{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 4: Hadoop for Fun and Profit \n",
    "Conor Meade\\\n",
    "CS 119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Functional Programming [25 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) add(), sub(), and ra_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "33\n",
      "6\n",
      "2\n",
      "6\n",
      "5\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "def add(*num_list):\n",
    "    sum = (functools.reduce(lambda x, y: x+y, num_list))\n",
    "    return sum\n",
    "\n",
    "print(add(1, 55, 45))\n",
    "print(add(0, 1, 1, 2, 3, 5, 8, 13))\n",
    "print(add(1, 2, 3))\n",
    "\n",
    "def sub(*num_list):\n",
    "    difference = (functools.reduce(lambda x, y: x-y, num_list))\n",
    "    return difference\n",
    "\n",
    "print(sub(5,1,2))\n",
    "\n",
    "def ra_sub(*num_list):\n",
    "    difference = (functools.reduce(lambda x, y: y - x, reversed(num_list)))\n",
    "    return difference\n",
    "\n",
    "print(ra_sub(5, 1, 2))\n",
    "print(ra_sub(5))\n",
    "print(ra_sub(5, 1, 2, 4, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2]]\n",
      "[[1, 4], [2, 5], [3, 6]]\n",
      "[[1, 4, 7], [2, 5, 8], [3, 6, 9]]\n"
     ]
    }
   ],
   "source": [
    "def zip(*num_sequences):\n",
    "    zipped_lists = [list(map(lambda s: s[i], num_sequences)) for i in range(len(num_sequences[0]))]\n",
    "    return zipped_lists\n",
    "\n",
    "print(zip([1], [2]))\n",
    "print(zip([1, 2, 3], [4, 5, 6]))\n",
    "print(zip([1, 2, 3], [4, 5, 6], [7, 8, 9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) zipwith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 9]\n",
      "[6, 8, 10]\n",
      "[-4, -4, -4]\n"
     ]
    }
   ],
   "source": [
    "def zipwith(func, *num_sequences):\n",
    "    result = list(map(lambda *args: func(*args), *num_sequences))\n",
    "    return result\n",
    "\n",
    "print(zipwith(add, [1, 2, 3], [4, 5, 6]))  # [5, 7, 9]\n",
    "print(zipwith(add, [1, 2, 3], [4, 5, 6], [1, 1, 1]))\n",
    "print(zipwith(sub, [1, 2, 3], [4, 5, 6], [1, 1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[2, 3, 4, 6, 4, 4, 1, 2, 3, 4, 7, 99]\n"
     ]
    }
   ],
   "source": [
    "def flatten(*tree):\n",
    "    flat_list = functools.reduce(lambda acc, node: acc + flatten(*node) if isinstance(node, list) else acc + [node], tree, [])\n",
    "    # flat_list = functools.reduce(lambda x,y: x+y, tree)\n",
    "    return flat_list\n",
    "\n",
    "print(flatten([1, [2, [3, 4], [5, 6], 7], 8, [9, 10]]))\n",
    "print(flatten([[2, 3, 4], 6, [4, 4], [[1, 2], 3, [4, 7, 99]]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.5) group_by()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: ['hi', 'me'], 3: ['dog', 'bad'], 4: ['good']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_by(func, num_sequences):\n",
    "    func_dict = {}\n",
    "\n",
    "    for s in num_sequences:\n",
    "        k = func(s)\n",
    "        if k in func_dict.keys():\n",
    "            func_dict[k].append(s)\n",
    "        else:\n",
    "            func_dict[k] = [s]\n",
    "    return func_dict\n",
    "\n",
    "\n",
    "group_by(len, [\"hi\", \"dog\", \"me\", \"bad\", \"good\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Confirming Hadoop Installation [15 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Acquire the cluster\n",
    "\n",
    "![Acquire Cluster](Part2/create_cluster.jpeg)\n",
    "\n",
    "After changing settings to allow for any ip to access, not just internal, and inputting the settings that are provided in Professor J's directions, I was able to create and run my cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2)  Load the data into the master, move the data into HDFS\n",
    "\n",
    "![Load Data, move data into HDFS](Part2/MoveFilesHDFS.png)\n",
    "\n",
    "First I SSH'd into my clusted confirmed my hadoop version (`hadoop version`) and cloned the repo (`git clone https://github.com/singhj/big-data-repo.git`). These both ran fine and ouputted what was expected. Then I was able to use mkdir to create my directories in the hadoop fs. No errors here and ls returned this new directoires so that worked fine. `hadoop fs -put ~/big-data-repo/five-books/* /user/singhj/five-books` put the five-books data into my hadoop file system and `hadoop fs -ls /user/singhj/five-books` allowed me to confirm that all five books of data made it into my cluster. Ouput can be seen in attached picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3)  Without writing any code of your own, verify that you have a good installation of hadoop by running wordcount on five-books. The command is similar to...\n",
    "![Books Count 1](Part2/books_count_1.png)\n",
    "![Books Count 2](Part2/books_count_2.png)\n",
    "\n",
    "Running `hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /user/singhj/five-books /books-count` was used to confirm I had a good installation of hadoop. This process showed a mapping and reduce function applied to the five-books data in order to confirm Hadoop working. The output can be seen in the attached pictures. We can see a mapreduce job percentage completion breakdown in the middle of the first picture along with a successful MR job completed message. I then fetch the /books-count directory using `hadoop fs -get /books-count`. I return the results using `ls -la books-count/`. This looks to have worked, it returns:\n",
    "\n",
    "```\n",
    "total 320\n",
    "drwxr-xr-x  2 cmeade6479 cmeade6479   4096 Oct 12 17:23 .\n",
    "drwxr-xr-x 12 cmeade6479 cmeade6479   4096 Oct 12 17:23 ..\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479      0 Oct 12 17:23 _SUCCESS\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 105799 Oct 12 17:23 part-r-00000\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 103061 Oct 12 17:23 part-r-00001\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 104969 Oct 12 17:23 part-r-00002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.4)  Run wordcount using the provided mapper_noll.py and the default reducer aggregate\n",
    "\n",
    "![mapper 1](Part2/mapred_1.png)\n",
    "![mapper 2](Part2/mapred_2.png)\n",
    "![mapper 3](Part2/mapred_3.png)\n",
    "\n",
    "Similar to the last question, I apply the commands seen in the directions and was able to complete the MR tasks without issues. I fetch the results using  `hadoop fs -get /books-stream-count` and that runs without error. Running `ls -la books-stream-count/` to confirm word count is working and it looks like it does. That command returns a success message:\n",
    "\n",
    "```\n",
    "total 116\n",
    "drwxr-xr-x  2 cmeade6479 cmeade6479  4096 Oct 12 17:27 .\n",
    "drwxr-xr-x 13 cmeade6479 cmeade6479  4096 Oct 12 17:27 ..\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479     0 Oct 12 17:27 _SUCCESS\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 34743 Oct 12 17:27 part-00000\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 34964 Oct 12 17:27 part-00001\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 33989 Oct 12 17:27 part-00002\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.5)  Run wordcount using the provided mapper_noll.py and the provided reducer reducer_noll.py\n",
    "\n",
    "![mapper 4](Part2/mapred_4.png)\n",
    "![mapper 5](Part2/mapred_5.png)\n",
    "![mapper 6](Part2/mapred_6.png)\n",
    "\n",
    "Similar to the previous two questions, I apply the commands seen in the directions and was able to complete the MR tasks without big issues. There was a small issue where the `-files` tag was not working so I used `-file` like in the last question and was able to run the command without issue. I fetch the results using  `hadoop fs -get /books-my-own-counts` and that runs without error. Running `ls -la books-my-own-counts/` to confirm word count is working and it looks like it does. That command returns a success message:\n",
    "\n",
    "```\n",
    "total 244\n",
    "drwxr-xr-x  2 cmeade6479 cmeade6479  4096 Oct 12 17:31 .\n",
    "drwxr-xr-x 14 cmeade6479 cmeade6479  4096 Oct 12 17:31 ..\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479     0 Oct 12 17:31 _SUCCESS\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 79255 Oct 12 17:31 part-00000\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 79515 Oct 12 17:31 part-00001\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 77539 Oct 12 17:31 part-00002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing Server Logs [55 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1)  What is the percentage of each request type (GET, PUT, POST, etc.)\n",
    "\n",
    "For this question, I wrote my own mapper and reduce functions, `request-type-mapper.py` and `request-type-reducer-count.py`. The mapper will return each request type along with a 1 for each row with that request type. This will be like: \\\n",
    "\n",
    "GET 1 \\\n",
    "POST 1 \\\n",
    "GET 1 \\\n",
    "GET 1 \\\n",
    "POST 1 \\\n",
    "HEAD 1 \\\n",
    "...\n",
    "\n",
    "For the reducer function, it will take these line by line counts of 1 and return a count value for each request.  \n",
    "I clone the repo for this project using `git clone https://github.com/ConorMeade/Quiz4`. \n",
    "\n",
    "To use the log file, mapper, and reducer, I have to load all the files in the Hadoop file system:\n",
    "```console\n",
    "$hadoop fs -mkdir /user/cmeade/Quiz4\n",
    "```\n",
    "\n",
    "```console\n",
    "$hadoop fs -put ~/Quiz4/* /user/cmeade/Quiz4\n",
    "```\n",
    "\n",
    "Next, run the map reduce command\n",
    "\n",
    "```console\n",
    "$mapred streaming -file ~/Quiz4/request-type-mapper.py ~/Quiz4/request-type-reducer-count.py \\\n",
    "-mapper request-type-mapper.py   \\\n",
    "-reducer request-type-reducer-count.py \\\n",
    "-input /user/cmeade/access.log.txt \\\n",
    "-output /request-type-counts\n",
    "```\n",
    "\n",
    "`/request-type-counts` will have the part output files\n",
    "```console\n",
    "$hdfs dfs -ls /request-type-counts\n",
    "```\n",
    "Found 4 items \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop          0 2024-10-13 00:52 /request-type-counts/_SUCCESS \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop         21 2024-10-13 00:52 /request-type-counts/part-00000 \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop          0 2024-10-13 00:52 /request-type-counts/part-00001 \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop          9 2024-10-13 00:52 /request-type-counts/part-00002\n",
    "\n",
    "\n",
    "The counts are split acoss different parts (00000 and 00002). So, combine all the part output files and output that into a text file req_counts\n",
    "```console\n",
    "$hdfs dfs -text /request-type-counts/part* > req_counts.txt\n",
    "```\n",
    "\n",
    "```text\n",
    "<!-- req_counts.txt -->\n",
    "GET     33414\n",
    "POST    44584\n",
    "HEAD    253\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Process Step\n",
    "\n",
    "The request-type-reducer-count reducer file will give the text ouput of the counts. Use get_percentages_req() post processing to determine the percentage of each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET\t33414\t42.70%\n",
      "POST\t44584\t56.98%\n",
      "HEAD\t253\t0.32%\n"
     ]
    }
   ],
   "source": [
    "def get_percentages_req():\n",
    "    total_reqs = 0\n",
    "    request_percentages = {}\n",
    "    with open('req_counts.txt', 'r') as c:\n",
    "        for line in c:\n",
    "            # print(line)\n",
    "            if line is not None or line != '\\n':\n",
    "                # varrying number of spaces so split on space and remove list elems that are empty/only spaces\n",
    "                line_list = line.strip().split(' ')\n",
    "                request_type, count = [item for item in line_list if item.strip() != \"\"]\n",
    "                count = int(count)\n",
    "                request_percentages[request_type] = count\n",
    "                total_reqs += count\n",
    "\n",
    "    # calculate percentage\n",
    "    for request_type, count in request_percentages.items():\n",
    "        percentage = (count / total_reqs) * 100\n",
    "        print(f\"{request_type}\\t{count}\\t{percentage:.2f}%\")\n",
    "\n",
    "get_percentages_req()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper and Reducer functions\n",
    "\n",
    "Here are the mapper and reducer functions used in map reduce processing. mapper() used for debugging locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''request-type-mapper.py'''\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    line = sys.stdin.readline()\n",
    "    # regex to get request type after opening quotation in access.log\n",
    "    pattern = re.compile(r'\\\"(\\w+)\\s')\n",
    "    try:\n",
    "        while line:\n",
    "            match = pattern.findall(line)\n",
    "            if match:\n",
    "                method = match[0] # Extract the request method (e.g., GET, POST, HEAD)\n",
    "                print(f\"{method}\\t1\")\n",
    "            line = sys.stdin.readline()\n",
    "                \n",
    "    except EOFError as error:\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)\n",
    "\n",
    "\n",
    "def mapper(debug=False):\n",
    "    # actually see what function returns\n",
    "    output_filename = 'mapper_output.txt'\n",
    "    with open(output_filename, 'w') as output_file:\n",
    "        with open('access.log.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                # line = sys.stdin.readline()\n",
    "                pattern = re.compile(r'\\\"(\\w+)\\s')\n",
    "                match = pattern.findall(line)\n",
    "                if match:\n",
    "                    method = match[0]  # Extract the request method (e.g., GET, POST)\n",
    "                    output_file.write(f\"{method} 1\\n\")\n",
    "                    # print(f\"{method}\\t1\")\n",
    "\n",
    "mapper(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET\t42.70%\n",
      "POST\t56.98%\n",
      "HEAD\t0.32%\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "'''request-type-reducer-count.py'''\n",
    "import sys\n",
    "\n",
    "# build dictionary element for distinct request type with the accompanying counts\n",
    "request_count = {}\n",
    "for line in sys.stdin:\n",
    "    method, count = line.strip().split('\\t', 1)\n",
    "    try:\n",
    "        count  = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if method not in request_count:\n",
    "        request_count[method] = 0\n",
    "\n",
    "    request_count[method] += count\n",
    "\n",
    "for method, count in request_count.items():\n",
    "    print(f\"{method}\\t{count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2) What percent of the responses fall into each of the following five types?\n",
    "For this question, I wrote my own mapper and reduce functions, `request-code-mapper.py` and `request-code-reducer-count.py`. The mapper will return each request type along with a 1 for each row with that request type. This will be like: \\\n",
    " 200\t1 \\\n",
    " 200\t1 \\\n",
    " 200\t1 \\\n",
    " 200\t1 \\\n",
    " 200\t1 \\\n",
    " 301\t1 \\\n",
    " 200\t1 \\\n",
    " 301\t1 \\\n",
    " 301\t1 \\\n",
    " 404\t1 \\\n",
    " 301\t1 \\\n",
    " 200\t1 \n",
    "\n",
    "\n",
    "\n",
    "To use the log file, mapper, and reducer, I have to load all the files in the Hadoop file system:\n",
    "```console\n",
    "$hadoop fs -mkdir /user/cmeade/Quiz4\n",
    "```\n",
    "\n",
    "```console\n",
    "$hadoop fs -put ~/Quiz4/* /user/cmeade/Quiz4\n",
    "```\n",
    "\n",
    "Next, run the map reduce command\n",
    "\n",
    "```console\n",
    "$mapred streaming -file ~/Quiz4/request-code-mapper.py ~/Quiz4/request-code-reducer-count.py \\\n",
    "-mapper request-code-mapper.py   \\\n",
    "-reducer request-code-reducer-count.py \\\n",
    "-input /user/cmeade/access.log.txt \\\n",
    "-output /request-code-counts\n",
    "```\n",
    "\n",
    "`/request-code-counts` will have the part output files\n",
    "```console\n",
    "$hdfs dfs -ls /request-code-counts\n",
    "```\n",
    "Found 4 items \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop          0 2024-10-13 02:39 /request-code-counts/_SUCCESS \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop         15 2024-10-13 02:39 /request-code-counts/part-00000 \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop         29 2024-10-13 02:39 /request-code-counts/part-00001 \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop         27 2024-10-13 02:39 /request-code-counts/part-00002\n",
    "\n",
    "\n",
    "The counts are split acoss different parts (00000, 00001, and 00002). So, combine all the part output files and output that into a text file code_counts.\n",
    "```console\n",
    "$hdfs dfs -text /request-code-counts/part* > code_counts.txt\n",
    "```\n",
    "```text\n",
    "<!-- code_counts.txt -->\n",
    "303     1857\n",
    "405     1\n",
    "301     957\n",
    "304     115\n",
    "400     1\n",
    "403     63\n",
    "200     70559\n",
    "206     125\n",
    "404     4573\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Process Step\n",
    "\n",
    "The request-code-reducer-count reducer file will give the text ouput of the counts. Use get_percentages_code() post processing to determine the percentage of each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informational responses (100–199)\t0.00%\n",
      "Successful responses (200–299)\t90.33%\n",
      "Redirection messages (300–399)\t3.74%\n",
      "Client error responses (400–499)\t5.93%\n",
      "Server error responses (500–599)\t0.00%\n"
     ]
    }
   ],
   "source": [
    "def get_percentages_code():\n",
    "    response_code_counts = {\n",
    "        \"Informational responses (100–199)\": 0,\n",
    "        \"Successful responses (200–299)\": 0, \n",
    "        \"Redirection messages (300–399)\": 0,\n",
    "        \"Client error responses (400–499)\": 0,\n",
    "        \"Server error responses (500–599)\": 0\n",
    "    }\n",
    "\n",
    "    with open('code_counts.txt', 'r') as c:\n",
    "        for line in c:\n",
    "            # print(line)\n",
    "            if line is not None or line != '\\n':\n",
    "                # varrying number of spaces so split on space and remove list elems that are empty/only spaces\n",
    "                line_list = line.strip().split(' ')\n",
    "                response_code, count = [item for item in line_list if item.strip() != \"\"]\n",
    "                try:\n",
    "                    response_code = int(response_code)\n",
    "                    count = int(count)\n",
    "                except ValueError:\n",
    "                    # count or response_code was not a number, so silently\n",
    "                    # ignore/discard this line\n",
    "                    continue\n",
    "                if 100 <= response_code < 200:\n",
    "                    response_code_counts[\"Informational responses (100–199)\"] += count\n",
    "                elif 200 <= response_code < 300:\n",
    "                    response_code_counts[\"Successful responses (200–299)\"] += count\n",
    "                elif 300 <= response_code < 400:\n",
    "                    response_code_counts[\"Redirection messages (300–399)\"] += count\n",
    "                elif 400 <= response_code < 500:\n",
    "                    response_code_counts[\"Client error responses (400–499)\"] += count\n",
    "                elif 500 <= response_code < 600:\n",
    "                    response_code_counts[\"Server error responses (500–599)\"] += count\n",
    "                else:\n",
    "                    print(f\"Unknown Code reached {response_code}\")\n",
    "\n",
    "                \n",
    "                total_reqs = sum(response_code_counts.values())\n",
    "\n",
    "    for response, count in response_code_counts.items():\n",
    "        percentage = (count / total_reqs) * 100\n",
    "        print(f\"{response}\\t{percentage:.2f}%\")\n",
    "\n",
    "get_percentages_code()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper and Reducer functions\n",
    "\n",
    "Here are the mapper and reducer functions used in map reduce processing. reducer function is pprtty much the same except for changing variable names to account for request values vs return code values. mapper_code() used for debugging locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "'''request-code-mapper.py'''\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    line = sys.stdin.readline()\n",
    "    pattern = re.compile(r'\\\" \\d{3}')\n",
    "    try:\n",
    "        while line:\n",
    "            match = pattern.findall(line)\n",
    "            if match:\n",
    "                method = match[0] # Extract the request method (e.g., GET, POST, HEAD)\n",
    "                print(f\"{method}\\t1\")\n",
    "            line = sys.stdin.readline()\n",
    "                \n",
    "    except EOFError as error:\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)\n",
    "\n",
    "def mapper_code(debug=False):\n",
    "    # actually see what function returns\n",
    "    output_filename = 'mapper_log_code_output.txt'\n",
    "    with open(output_filename, 'w') as output_file:\n",
    "        with open('access.log.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                # line = sys.stdin.readline()\n",
    "                pattern = re.compile(r'\\ \\d{3}')\n",
    "                match = pattern.findall(line)\n",
    "                if match:\n",
    "                    response_code = match[0]  # Extract the request response_code (e.g., 200, 400, 401, 500)\n",
    "                    output_file.write(f\"{response_code}\\t1\\n\")\n",
    "                    # print(f\"{method}\\t1\")\n",
    "\n",
    "mapper_code(debug=True)\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "'''request-code-reducer-count.py'''\n",
    "import sys\n",
    "\n",
    "request_count = {}\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    code, count = line.strip().split('\\t', 1)\n",
    "    try:\n",
    "        count  = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if code not in request_count:\n",
    "        request_count[code] = 0\n",
    "\n",
    "    request_count[code] += count\n",
    "\n",
    "for code, count in request_count.items():\n",
    "    print(f\"{code}\\t{count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.3) What 5 IP addresses generate the most client errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Presidential Speeches [15 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "stopwords = list(set(stopwords_list.decode().splitlines()))\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    list_ = re.sub(r\"[^a-zA-Z0-9]\", \" \", words.lower()).split()\n",
    "    return [itm for itm in list_ if itm not in stopwords]\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('[\\d\\n]', ' ', text)\n",
    "    return ' '.join(remove_stopwords(text))\n",
    "\n",
    "\n",
    "def valence(text):\n",
    "    pass\n",
    "\n",
    "def calc_valence(text):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
