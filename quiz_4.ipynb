{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 4: Hadoop for Fun and Profit \n",
    "Conor Meade\\\n",
    "CS 119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Functional Programming [25 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) add(), sub(), and ra_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "33\n",
      "6\n",
      "2\n",
      "6\n",
      "5\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "def add(*num_list):\n",
    "    sum = (functools.reduce(lambda x, y: x+y, num_list))\n",
    "    return sum\n",
    "\n",
    "print(add(1, 55, 45))\n",
    "print(add(0, 1, 1, 2, 3, 5, 8, 13))\n",
    "print(add(1, 2, 3))\n",
    "\n",
    "def sub(*num_list):\n",
    "    difference = (functools.reduce(lambda x, y: x-y, num_list))\n",
    "    return difference\n",
    "\n",
    "print(sub(5,1,2))\n",
    "\n",
    "def ra_sub(*num_list):\n",
    "    difference = (functools.reduce(lambda x, y: y - x, reversed(num_list)))\n",
    "    return difference\n",
    "\n",
    "print(ra_sub(5, 1, 2))\n",
    "print(ra_sub(5))\n",
    "print(ra_sub(5, 1, 2, 4, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2]]\n",
      "[[1, 4], [2, 5], [3, 6]]\n",
      "[[1, 4, 7], [2, 5, 8], [3, 6, 9]]\n"
     ]
    }
   ],
   "source": [
    "def zip(*num_sequences):\n",
    "    zipped_lists = [list(map(lambda s: s[i], num_sequences)) for i in range(len(num_sequences[0]))]\n",
    "    return zipped_lists\n",
    "\n",
    "print(zip([1], [2]))\n",
    "print(zip([1, 2, 3], [4, 5, 6]))\n",
    "print(zip([1, 2, 3], [4, 5, 6], [7, 8, 9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) zipwith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 9]\n",
      "[6, 8, 10]\n",
      "[-4, -4, -4]\n"
     ]
    }
   ],
   "source": [
    "def zipwith(func, *num_sequences):\n",
    "    result = list(map(lambda *args: func(*args), *num_sequences))\n",
    "    return result\n",
    "\n",
    "print(zipwith(add, [1, 2, 3], [4, 5, 6]))  # [5, 7, 9]\n",
    "print(zipwith(add, [1, 2, 3], [4, 5, 6], [1, 1, 1]))\n",
    "print(zipwith(sub, [1, 2, 3], [4, 5, 6], [1, 1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[2, 3, 4, 6, 4, 4, 1, 2, 3, 4, 7, 99]\n"
     ]
    }
   ],
   "source": [
    "def flatten(*tree):\n",
    "    flat_list = functools.reduce(lambda acc, node: acc + flatten(*node) if isinstance(node, list) else acc + [node], tree, [])\n",
    "    # flat_list = functools.reduce(lambda x,y: x+y, tree)\n",
    "    return flat_list\n",
    "\n",
    "print(flatten([1, [2, [3, 4], [5, 6], 7], 8, [9, 10]]))\n",
    "print(flatten([[2, 3, 4], 6, [4, 4], [[1, 2], 3, [4, 7, 99]]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.5) group_by()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: ['hi', 'me'], 3: ['dog', 'bad'], 4: ['good']}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_by(func, num_sequences):\n",
    "    func_dict = {}\n",
    "\n",
    "    for s in num_sequences:\n",
    "        k = func(s)\n",
    "        if k in func_dict.keys():\n",
    "            func_dict[k].append(s)\n",
    "        else:\n",
    "            func_dict[k] = [s]\n",
    "    return func_dict\n",
    "\n",
    "\n",
    "group_by(len, [\"hi\", \"dog\", \"me\", \"bad\", \"good\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Confirming Hadoop Installation [15 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Acquire the cluster\n",
    "\n",
    "![Acquire Cluster](Part2/create_cluster.jpeg)\n",
    "\n",
    "After changing settings to allow for any ip to access, not just internal, and inputting the settings that are provided in Professor J's directions, I was able to create and run my cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2)  Load the data into the master, move the data into HDFS\n",
    "\n",
    "![Load Data, move data into HDFS](Part2/MoveFilesHDFS.png)\n",
    "\n",
    "First I SSH'd into my clusted confirmed my hadoop version (`hadoop version`) and cloned the repo (`git clone https://github.com/singhj/big-data-repo.git`). These both ran fine and ouputted what was expected. Then I was able to use mkdir to create my directories in the hadoop fs. No errors here and ls returned this new directoires so that worked fine. `hadoop fs -put ~/big-data-repo/five-books/* /user/singhj/five-books` put the five-books data into my hadoop file system and `hadoop fs -ls /user/singhj/five-books` allowed me to confirm that all five books of data made it into my cluster. Ouput can be seen in attached picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3)  Without writing any code of your own, verify that you have a good installation of hadoop by running wordcount on five-books. The command is similar to...\n",
    "![Books Count 1](Part2/books_count_1.png)\n",
    "![Books Count 2](Part2/books_count_2.png)\n",
    "\n",
    "Running `hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /user/singhj/five-books /books-count` was used to confirm I had a good installation of hadoop. This process showed a mapping and reduce function applied to the five-books data in order to confirm Hadoop working. The output can be seen in the attached pictures. We can see a mapreduce job percentage completion breakdown in the middle of the first picture along with a successful MR job completed message. I then fetch the /books-count directory using `hadoop fs -get /books-count`. I return the results using `ls -la books-count/`. This looks to have worked, it returns:\n",
    "\n",
    "```\n",
    "total 320\n",
    "drwxr-xr-x  2 cmeade6479 cmeade6479   4096 Oct 12 17:23 .\n",
    "drwxr-xr-x 12 cmeade6479 cmeade6479   4096 Oct 12 17:23 ..\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479      0 Oct 12 17:23 _SUCCESS\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 105799 Oct 12 17:23 part-r-00000\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 103061 Oct 12 17:23 part-r-00001\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 104969 Oct 12 17:23 part-r-00002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.4)  Run wordcount using the provided mapper_noll.py and the default reducer aggregate\n",
    "\n",
    "![mapper 1](Part2/mapred_1.png)\n",
    "![mapper 2](Part2/mapred_2.png)\n",
    "![mapper 3](Part2/mapred_3.png)\n",
    "\n",
    "Similar to the last question, I apply the commands seen in the directions and was able to complete the MR tasks without issues. I fetch the results using  `hadoop fs -get /books-stream-count` and that runs without error. Running `ls -la books-stream-count/` to confirm word count is working and it looks like it does. That command returns a success message:\n",
    "\n",
    "```\n",
    "total 116\n",
    "drwxr-xr-x  2 cmeade6479 cmeade6479  4096 Oct 12 17:27 .\n",
    "drwxr-xr-x 13 cmeade6479 cmeade6479  4096 Oct 12 17:27 ..\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479     0 Oct 12 17:27 _SUCCESS\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 34743 Oct 12 17:27 part-00000\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 34964 Oct 12 17:27 part-00001\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 33989 Oct 12 17:27 part-00002\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.5)  Run wordcount using the provided mapper_noll.py and the provided reducer reducer_noll.py\n",
    "\n",
    "![mapper 4](Part2/mapred_4.png)\n",
    "![mapper 5](Part2/mapred_5.png)\n",
    "![mapper 6](Part2/mapred_6.png)\n",
    "\n",
    "Similar to the previous two questions, I apply the commands seen in the directions and was able to complete the MR tasks without big issues. There was a small issue where the `-files` tag was not working so I used `-file` like in the last question and was able to run the command without issue. I fetch the results using  `hadoop fs -get /books-my-own-counts` and that runs without error. Running `ls -la books-my-own-counts/` to confirm word count is working and it looks like it does. That command returns a success message:\n",
    "\n",
    "```\n",
    "total 244\n",
    "drwxr-xr-x  2 cmeade6479 cmeade6479  4096 Oct 12 17:31 .\n",
    "drwxr-xr-x 14 cmeade6479 cmeade6479  4096 Oct 12 17:31 ..\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479     0 Oct 12 17:31 _SUCCESS\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 79255 Oct 12 17:31 part-00000\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 79515 Oct 12 17:31 part-00001\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 77539 Oct 12 17:31 part-00002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing Server Logs [55 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1)  What is the percentage of each request type (GET, PUT, POST, etc.)\n",
    "\n",
    "For this question, I wrote my own mapper and reduce functions, `request-type-mapper.py` and `request-type-reducer.py`. The mapper will return each request type along with a 1 for each row with that request type. This will be like:\n",
    "GET 1\n",
    "POST 1\n",
    "PUT 1\n",
    "...\n",
    "\n",
    "For the reducer function, it will take these line by line counts and return a percentage value for each request. We have what is needed to run MR for this. I clone the repo for this project using `git clone https://github.com/ConorMeade/Quiz4`. To use the log file, mapper, and reducer, I have to load all the files in the Hadoop file system:\n",
    "\n",
    "hdfs dfs -text /request-type-percentages/p*\n",
    "\n",
    "`hadoop fs -mkdir /user/cmeade/Quiz4`\n",
    "`hadoop fs -put ~/Quiz4/* /user/cmeade/Quiz4`\n",
    "\n",
    "\n",
    "Gather combinations of all files in counts:\n",
    "hdfs dfs -text /request-type-counts/p*\n",
    "\n",
    "remove dir:\n",
    "\n",
    "hadoop fs -rm -r /request-type-percentages\n",
    "Then I run the mapreduce command:\n",
    "\n",
    "\n",
    "hadoop jar /usr/lib/hadoop/hadoop-streaming-3.3.6.jar\n",
    "\n",
    "hadoop jar /usr/lib/hadoop/hadoop-streaming-3.3.6.jar \\\n",
    "-input /user/cmeade/access.log.txt \\\n",
    "-output /request-type-percentages \\\n",
    "-mapper request-type-mapper.py \\\n",
    "-reducer request_type-reducer.py \\\n",
    "-file ~/Quiz4/request-type-mapper.py  \\\n",
    "-file ~/Quiz4/request-type-reducer.py \n",
    "\n",
    "\n",
    "## GETTING COUNTS\n",
    "```\n",
    "mapred streaming -file ~/Quiz4/request-type-mapper.py ~/Quiz4/request-type-reducer-count.py \\\n",
    "-mapper request-type-mapper.py   \\\n",
    "-reducer request-type-reducer-count.py \\\n",
    "-input /user/cmeade/access.log.txt \\\n",
    "-output /request-type-counts\n",
    "```\n",
    "## GETTING PERCENTAGES\n",
    "\n",
    "```\n",
    "hdfs dfs -text /request-type-counts/p* > /request-type-counts/request_cts.txt \n",
    "\n",
    "\n",
    "mapred streaming -file ~/Quiz4/request-type-reducer-percentage.py \\\n",
    "-mapper cat \\\n",
    "-reducer request-type-reducer-percentage.py \\\n",
    "-input /user/cmeade/counts.txt \\\n",
    "-output /request-type-percentage \n",
    "```\n",
    "## PUT COUNTS IN HDFS\n",
    "hdfs dfs -text /request-type-counts/p* > counts.txt \n",
    "hadoop fs -put ~/Quiz4/counts.txt /user/cmeade\n",
    "\n",
    "\n",
    "rm request_cts & hadoop fs -rm -r /request-type-counts\n",
    "\n",
    "```\n",
    "\n",
    "<!-- hadoop jar ./hadoop-streaming-3.1.4.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-file ~/Quiz4/request-type-mapper.py \\\n",
    "-mapper request-type-mapper.py  \\\n",
    "-file ~/Quiz4/request-type-reducer.py\\\n",
    "-reducer request-type-reducer.py \\\n",
    "-input /user/cmeade/access.log.txt  \\\n",
    "-output /request-type-percentages -->\n",
    "\n",
    "\n",
    "```\n",
    "mapred streaming -file user/cmeade/Quiz4/request-type-mapper.py user/cmeade/Quiz4/request-type-mapper.py \\\n",
    "-mapper request-type-mapper.py   \\\n",
    "-reducer request-type-reducer.py \\\n",
    "-input /user/cmeade/access.log.txt \\\n",
    "-output /request-type-percentages\n",
    "```\n",
    "\n",
    "```\n",
    "mapred streaming -file ~/Quiz4/request-type-mapper.py ~/big-data-repo/hadoop/reducer_noll.py \\\n",
    "-mapper request-type-mapper.py   \\\n",
    "-reducer reducer_noll.py \\\n",
    "-input /user/cmeade/access.log.txt \\\n",
    "-output /request-type-percentages\n",
    "```\n",
    "\n",
    "```\n",
    "hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "-input /user/cmeade/access.log.txt \\\n",
    "-output /request-type-percentages\n",
    "-mapper ~/Quiz4/request-type-mapper.py  \\\n",
    "-reducer ~/Quiz4/request-type-reducer.py \n",
    "```\n",
    "\n",
    "```\n",
    "mapred streaming \\\n",
    "-input /user/cmeade/access.log.txt \\\n",
    "-output /request-type-percentages\\\n",
    "-mapper \"python request-type-mapper.py\"  \\\n",
    "-reducer \"python request-type-reducer.py\"  \\\n",
    "-file ~/Quiz4/request-type-mapper.py \\\n",
    "-file ~/Quiz4/request-type-reducer.py \n",
    "```\n",
    "\n",
    "mapred streaming  -file ~Quiz4/request-type-mapper.py ~Quiz4/request-type-reducer.py \\\n",
    "-input /user/cmeade/access.log.txt \\\n",
    "-output /request-type-percentages\\\n",
    "-mapper \"python request-type-mapper.py\"  \\\n",
    "-reducer \"python request-type-reducer.py\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(debug=False):\n",
    "    # actually see what function returns\n",
    "    if debug:\n",
    "        output_filename = 'mapper_output.txt'\n",
    "        with open(output_filename, 'w') as output_file:\n",
    "            with open('access.log.txt', 'r') as f:\n",
    "                for line in f:\n",
    "                    # line = sys.stdin.readline()\n",
    "                    pattern = re.compile(r'\\\"(\\w+)\\s')\n",
    "                    match = pattern.findall(line)\n",
    "                    if match:\n",
    "                        method = match[0]  # Extract the request method (e.g., GET, POST)\n",
    "                        output_file.write(f\"{method} 1\\n\")\n",
    "                        # print(f\"{method}\\t1\")\n",
    "\n",
    "    # need this format when actually running with hadoop\n",
    "    else:\n",
    "        for line in sys.stdin:\n",
    "            # split on opening parens, second element will be the request type\n",
    "            log_elems = line.split('\"')\n",
    "            if len(log_elems) > 1:\n",
    "                # print(log_elems)\n",
    "                # break\n",
    "                # after date time portion, we have the GET, POST, PUT etc. message\n",
    "                request = log_elems[1].split()  # ['GET', '/index.php?option=com_phocagallery&view=category&id=1:almhuette-raith&Itemid=53', 'HTTP/1.1']\n",
    "                # ensure we actually have data\n",
    "                if len(request) > 0:\n",
    "                    method = request[0]\n",
    "                    # print(f\"{method}\\t1\")\n",
    "\n",
    "mapper(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET\t42.70%\n",
      "POST\t56.98%\n",
      "HEAD\t0.32%\n"
     ]
    }
   ],
   "source": [
    "def reducer(debug=False):\n",
    "\n",
    "    if debug:\n",
    "        request_count = {}\n",
    "        with open('mapper_output.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                method, count = line.strip().split(' ', 1)\n",
    "                try:\n",
    "                    count  = int(count)\n",
    "                except ValueError:\n",
    "                    # count was not a number, so silently\n",
    "                    # ignore/discard this line\n",
    "                    continue\n",
    "                if method not in request_count:\n",
    "                    request_count[method] = 0\n",
    "\n",
    "                request_count[method] += count\n",
    "\n",
    "            total_reqs = sum(request_count.values())\n",
    "\n",
    "            for method, count in request_count.items():\n",
    "                percentage_of_reqs = (count / total_reqs) * 100\n",
    "                print(f\"{method}\\t{percentage_of_reqs:.2f}%\")\n",
    "\n",
    "    \n",
    "    else:\n",
    "        request_count = {}\n",
    "\n",
    "\n",
    "        for line in sys.stdin:\n",
    "            method, count = line.strip().split('\\t')\n",
    "            request_count[method] += int(count)\n",
    "\n",
    "        total_reqs = sum(request_count.values())\n",
    "\n",
    "        for method, count in request_count.items():\n",
    "            percentage_of_reqs = (count / total_reqs) * 100\n",
    "            print(f\"{method}\\t{percentage_of_reqs:.2f}%\")\n",
    "\n",
    "    # return total_reqs\n",
    "\n",
    "reducer(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2) What percent of the responses fall into each of the following five types?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.3) What 5 IP addresses generate the most client errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Presidential Speeches [15 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "stopwords = list(set(stopwords_list.decode().splitlines()))\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    list_ = re.sub(r\"[^a-zA-Z0-9]\", \" \", words.lower()).split())\n",
    "    return [itm for itm in list_ if itm not in stopwords]\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('[\\d\\n]', ' ', text)\n",
    "    return ' '.join(remove_stopwords(text))\n",
    "\n",
    "\n",
    "def valence(text):\n",
    "    pass\n",
    "\n",
    "def calc_valence(text):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
