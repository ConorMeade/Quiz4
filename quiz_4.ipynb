{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 4: Hadoop for Fun and Profit \n",
    "Conor Meade\\\n",
    "CS 119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import requests\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Functional Programming [25 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) add(), sub(), and ra_sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "33\n",
      "6\n",
      "2\n",
      "6\n",
      "5\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "def add(*num_list):\n",
    "    sum = (functools.reduce(lambda x, y: x+y, num_list))\n",
    "    return sum\n",
    "\n",
    "print(add(1, 55, 45))\n",
    "print(add(0, 1, 1, 2, 3, 5, 8, 13))\n",
    "print(add(1, 2, 3))\n",
    "\n",
    "def sub(*num_list):\n",
    "    difference = (functools.reduce(lambda x, y: x-y, num_list))\n",
    "    return difference\n",
    "\n",
    "print(sub(5,1,2))\n",
    "\n",
    "def ra_sub(*num_list):\n",
    "    difference = (functools.reduce(lambda x, y: y - x, reversed(num_list)))\n",
    "    return difference\n",
    "\n",
    "print(ra_sub(5, 1, 2))\n",
    "print(ra_sub(5))\n",
    "print(ra_sub(5, 1, 2, 4, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2]]\n",
      "[[1, 4], [2, 5], [3, 6]]\n",
      "[[1, 4, 7], [2, 5, 8], [3, 6, 9]]\n"
     ]
    }
   ],
   "source": [
    "def zip(*num_sequences):\n",
    "    zipped_lists = [list(map(lambda s: s[i], num_sequences)) for i in range(len(num_sequences[0]))]\n",
    "    return zipped_lists\n",
    "\n",
    "print(zip([1], [2]))\n",
    "print(zip([1, 2, 3], [4, 5, 6]))\n",
    "print(zip([1, 2, 3], [4, 5, 6], [7, 8, 9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) zipwith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 9]\n",
      "[6, 8, 10]\n",
      "[-4, -4, -4]\n"
     ]
    }
   ],
   "source": [
    "def zipwith(func, *num_sequences):\n",
    "    result = list(map(lambda *args: func(*args), *num_sequences))\n",
    "    return result\n",
    "\n",
    "print(zipwith(add, [1, 2, 3], [4, 5, 6]))  # [5, 7, 9]\n",
    "print(zipwith(add, [1, 2, 3], [4, 5, 6], [1, 1, 1]))\n",
    "print(zipwith(sub, [1, 2, 3], [4, 5, 6], [1, 1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[2, 3, 4, 6, 4, 4, 1, 2, 3, 4, 7, 99]\n"
     ]
    }
   ],
   "source": [
    "def flatten(*tree):\n",
    "    flat_list = functools.reduce(lambda acc, node: acc + flatten(*node) if isinstance(node, list) else acc + [node], tree, [])\n",
    "    # flat_list = functools.reduce(lambda x,y: x+y, tree)\n",
    "    return flat_list\n",
    "\n",
    "print(flatten([1, [2, [3, 4], [5, 6], 7], 8, [9, 10]]))\n",
    "print(flatten([[2, 3, 4], 6, [4, 4], [[1, 2], 3, [4, 7, 99]]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.5) group_by()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: ['hi', 'me'], 3: ['dog', 'bad'], 4: ['good']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def group_by(func, num_sequences):\n",
    "    func_dict = {}\n",
    "\n",
    "    for s in num_sequences:\n",
    "        k = func(s)\n",
    "        if k in func_dict.keys():\n",
    "            func_dict[k].append(s)\n",
    "        else:\n",
    "            func_dict[k] = [s]\n",
    "    return func_dict\n",
    "\n",
    "\n",
    "group_by(len, [\"hi\", \"dog\", \"me\", \"bad\", \"good\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Confirming Hadoop Installation [15 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Acquire the cluster\n",
    "\n",
    "![Acquire Cluster](Part2/create_cluster.jpeg)\n",
    "\n",
    "After changing settings to allow for any ip to access, not just internal, and inputting the settings that are provided in Professor J's directions, I was able to create and run my cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2)  Load the data into the master, move the data into HDFS\n",
    "\n",
    "![Load Data, move data into HDFS](Part2/MoveFilesHDFS.png)\n",
    "\n",
    "First I SSH'd into my clusted confirmed my hadoop version (`hadoop version`) and cloned the repo (`git clone https://github.com/singhj/big-data-repo.git`). These both ran fine and ouputted what was expected. Then I was able to use mkdir to create my directories in the hadoop fs. No errors here and ls returned this new directoires so that worked fine. `hadoop fs -put ~/big-data-repo/five-books/* /user/singhj/five-books` put the five-books data into my hadoop file system and `hadoop fs -ls /user/singhj/five-books` allowed me to confirm that all five books of data made it into my cluster. Ouput can be seen in attached picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3)  Without writing any code of your own, verify that you have a good installation of hadoop by running wordcount on five-books. The command is similar to...\n",
    "![Books Count 1](Part2/books_count_1.png)\n",
    "![Books Count 2](Part2/books_count_2.png)\n",
    "\n",
    "Running `hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /user/singhj/five-books /books-count` was used to confirm I had a good installation of hadoop. This process showed a mapping and reduce function applied to the five-books data in order to confirm Hadoop working. The output can be seen in the attached pictures. We can see a mapreduce job percentage completion breakdown in the middle of the first picture along with a successful MR job completed message. I then fetch the /books-count directory using `hadoop fs -get /books-count`. I return the results using `ls -la books-count/`. This looks to have worked, it returns:\n",
    "\n",
    "```\n",
    "total 320\n",
    "drwxr-xr-x  2 cmeade6479 cmeade6479   4096 Oct 12 17:23 .\n",
    "drwxr-xr-x 12 cmeade6479 cmeade6479   4096 Oct 12 17:23 ..\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479      0 Oct 12 17:23 _SUCCESS\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 105799 Oct 12 17:23 part-r-00000\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 103061 Oct 12 17:23 part-r-00001\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 104969 Oct 12 17:23 part-r-00002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.4)  Run wordcount using the provided mapper_noll.py and the default reducer aggregate\n",
    "\n",
    "![mapper 1](Part2/mapred_1.png)\n",
    "![mapper 2](Part2/mapred_2.png)\n",
    "![mapper 3](Part2/mapred_3.png)\n",
    "\n",
    "Similar to the last question, I apply the commands seen in the directions and was able to complete the MR tasks without issues. I fetch the results using  `hadoop fs -get /books-stream-count` and that runs without error. Running `ls -la books-stream-count/` to confirm word count is working and it looks like it does. That command returns a success message:\n",
    "\n",
    "```\n",
    "total 116\n",
    "drwxr-xr-x  2 cmeade6479 cmeade6479  4096 Oct 12 17:27 .\n",
    "drwxr-xr-x 13 cmeade6479 cmeade6479  4096 Oct 12 17:27 ..\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479     0 Oct 12 17:27 _SUCCESS\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 34743 Oct 12 17:27 part-00000\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 34964 Oct 12 17:27 part-00001\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 33989 Oct 12 17:27 part-00002\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.5)  Run wordcount using the provided mapper_noll.py and the provided reducer reducer_noll.py\n",
    "\n",
    "![mapper 4](Part2/mapred_4.png)\n",
    "![mapper 5](Part2/mapred_5.png)\n",
    "![mapper 6](Part2/mapred_6.png)\n",
    "\n",
    "Similar to the previous two questions, I apply the commands seen in the directions and was able to complete the MR tasks without big issues. There was a small issue where the `-files` tag was not working so I used `-file` like in the last question and was able to run the command without issue. I fetch the results using  `hadoop fs -get /books-my-own-counts` and that runs without error. Running `ls -la books-my-own-counts/` to confirm word count is working and it looks like it does. That command returns a success message:\n",
    "\n",
    "```\n",
    "total 244\n",
    "drwxr-xr-x  2 cmeade6479 cmeade6479  4096 Oct 12 17:31 .\n",
    "drwxr-xr-x 14 cmeade6479 cmeade6479  4096 Oct 12 17:31 ..\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479     0 Oct 12 17:31 _SUCCESS\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 79255 Oct 12 17:31 part-00000\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 79515 Oct 12 17:31 part-00001\n",
    "-rw-r--r--  1 cmeade6479 cmeade6479 77539 Oct 12 17:31 part-00002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing Server Logs [55 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1)  What is the percentage of each request type (GET, PUT, POST, etc.)\n",
    "\n",
    "For this question, I wrote my own mapper and reduce functions, `request-type-mapper.py` and `request-type-reducer-count.py`. The mapper will return each request type along with a 1 for each row with that request type. This will be like: \\\n",
    "\n",
    "GET 1 \\\n",
    "POST 1 \\\n",
    "GET 1 \\\n",
    "GET 1 \\\n",
    "POST 1 \\\n",
    "HEAD 1 \\\n",
    "...\n",
    "\n",
    "For the reducer function, it will take these line by line counts of 1 and return a count value for each request.  \n",
    "I clone the repo for this project using `git clone https://github.com/ConorMeade/Quiz4`. \n",
    "\n",
    "Next, run the map reduce command\n",
    "\n",
    "```console\n",
    "$ mapred streaming -file ~/Quiz4/request-type-mapper.py ~/Quiz4/request-type-reducer-count.py \\\n",
    "-mapper request-type-mapper.py   \\\n",
    "-reducer request-type-reducer-count.py \\\n",
    "-input /user/cmeade/access.log.txt \\\n",
    "-output /request-type-counts\n",
    "```\n",
    "\n",
    "`/request-type-counts` will have the part output files\n",
    "```console\n",
    "$ hdfs dfs -ls /request-type-counts\n",
    "```\n",
    "Found 4 items \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop          0 2024-10-13 00:52 /request-type-counts/_SUCCESS \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop         21 2024-10-13 00:52 /request-type-counts/part-00000 \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop          0 2024-10-13 00:52 /request-type-counts/part-00001 \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop          9 2024-10-13 00:52 /request-type-counts/part-00002\n",
    "\n",
    "\n",
    "The counts are split acoss different parts (00000 and 00002). So, combine all the part output files and output that into a text file req_counts\n",
    "```console\n",
    "$ hdfs dfs -text /request-type-counts/part* > req_counts.txt\n",
    "```\n",
    "\n",
    "```text\n",
    "<!-- req_counts.txt -->\n",
    "LongValueSum:GET     33414\n",
    "LongValueSum:POST    44584\n",
    "LongValueSum:HEAD    253\n",
    "```\n",
    "\n",
    "\n",
    "Clean up cluster space by deleting the directory since the txt file has been generated.\n",
    "```console\n",
    "$ hdfs dfs -rm -r /request-type-counts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Process Step (3.1)\n",
    "\n",
    "The request-type-reducer-count reducer file (see below) will give the text ouput of the counts. Use get_percentages_req() post processing to determine the percentage of each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET\t33414\t42.70%\n",
      "POST\t44584\t56.98%\n",
      "HEAD\t253\t0.32%\n"
     ]
    }
   ],
   "source": [
    "def get_percentages_req():\n",
    "    total_reqs = 0\n",
    "    request_percentages = {}\n",
    "    with open('req_counts.txt', 'r') as c:\n",
    "        for line in c:\n",
    "            # print(line)\n",
    "            if line is not None or line != '\\n':\n",
    "                # varrying number of spaces so split on space and remove list elems that are empty/only spaces\n",
    "                line_list = line.strip().split(' ')\n",
    "                request_type, count = [item for item in line_list if item.strip() != \"\"]\n",
    "                count = int(count)\n",
    "                request_percentages[request_type] = count\n",
    "                total_reqs += count\n",
    "\n",
    "    # calculate percentage\n",
    "    for request_type, count in request_percentages.items():\n",
    "        percentage = (count / total_reqs) * 100\n",
    "        print(f\"{request_type}\\t{count}\\t{percentage:.2f}%\")\n",
    "\n",
    "get_percentages_req()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper and Reducer functions\n",
    "\n",
    "Here are the mapper and reducer functions used in map reduce processing. mapper() used for debugging locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAPPER (3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "'''request-type-mapper.py'''\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    line = sys.stdin.readline()\n",
    "    pattern = re.compile(r'\\\"(\\w+)\\s')\n",
    "    try:\n",
    "        while line:\n",
    "            match = pattern.findall(line)\n",
    "            if match:\n",
    "                method = match[0] # Extract the request method (e.g., GET, POST, HEAD)\n",
    "                print(f\"LongValueSum:{method}\\t1\")\n",
    "            line = sys.stdin.readline()\n",
    "                \n",
    "    except EOFError as error:\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)\n",
    "\n",
    "\n",
    "def mapper(debug=False):\n",
    "    # actually see what function returns, debug\n",
    "    output_filename = 'mapper_output.txt'\n",
    "    with open(output_filename, 'w') as output_file:\n",
    "        with open('access.log.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                # line = sys.stdin.readline()\n",
    "                pattern = re.compile(r'\\\"(\\w+)\\s')\n",
    "                match = pattern.findall(line)\n",
    "                if match:\n",
    "                    method = match[0]  # Extract the request method (e.g., GET, POST)\n",
    "                    output_file.write(f\"{method} 1\\n\")\n",
    "                    # print(f\"{method}\\t1\")\n",
    "\n",
    "mapper(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REDUCER (3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7020/1970626485.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "'''request-type-reducer-count.py'''\n",
    "import sys\n",
    "\n",
    "request_count = {}\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    method, count = line.strip().split('\\t', 1)\n",
    "    try:\n",
    "        count  = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if method not in request_count:\n",
    "        request_count[method] = 0\n",
    "\n",
    "    request_count[method] += count\n",
    "\n",
    "for method, count in request_count.items():\n",
    "    print(f\"{method}\\t{count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2) What percent of the responses fall into each of the following five types?\n",
    "For this question, I wrote my own mapper and reduce functions, `request-code-mapper.py` and `request-code-reducer-count.py`. The mapper will return each request type along with a 1 for each row with that request type. This will be like: \\\n",
    " 200\t1 \\\n",
    " 200\t1 \\\n",
    " 200\t1 \\\n",
    " 200\t1 \\\n",
    " 200\t1 \\\n",
    " 301\t1 \\\n",
    " 200\t1 \\\n",
    " 301\t1 \\\n",
    " 301\t1 \\\n",
    " 404\t1 \\\n",
    " 301\t1 \\\n",
    " 200\t1 \n",
    "\n",
    "\n",
    "\n",
    "To use the log file, mapper, and reducer, I have to pull them into the repo that exists in the cluster\n",
    "```console\n",
    "$ cd Quiz4\n",
    "$ git pull\n",
    "```\n",
    "\n",
    "Next, run the map reduce command\n",
    "\n",
    "```console\n",
    "$ mapred streaming -file ~/Quiz4/request-code-mapper.py ~/Quiz4/request-code-reducer-count.py \\\n",
    "-mapper request-code-mapper.py   \\\n",
    "-reducer request-code-reducer-count.py \\\n",
    "-input /user/cmeade/access.log.txt \\\n",
    "-output /request-code-counts\n",
    "```\n",
    "\n",
    "`/request-code-counts` will have the part output files\n",
    "```console\n",
    "$hdfs dfs -ls /request-code-counts\n",
    "```\n",
    "Found 4 items \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop          0 2024-10-13 02:39 /request-code-counts/_SUCCESS \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop         15 2024-10-13 02:39 /request-code-counts/part-00000 \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop         29 2024-10-13 02:39 /request-code-counts/part-00001 \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop         27 2024-10-13 02:39 /request-code-counts/part-00002\n",
    "\n",
    "\n",
    "The counts are split acoss different parts (00000, 00001, and 00002). So, combine all the part output files and output that into a text file code_counts.\n",
    "```console\n",
    "$ hdfs dfs -text /request-code-counts/part* > code_counts.txt\n",
    "```\n",
    "```text\n",
    "<!-- code_counts.txt -->\n",
    "LongValueSum:303     1857\n",
    "LongValueSum:405     1\n",
    "LongValueSum:301     957\n",
    "LongValueSum:304     115\n",
    "LongValueSum:400     1\n",
    "LongValueSum:403     63\n",
    "LongValueSum:200     70559\n",
    "LongValueSum:206     125\n",
    "LongValueSum:404     4573\n",
    "```\n",
    "\n",
    "\n",
    "Clean up cluster space by deleting the directory since the txt file has been generated.\n",
    "```console\n",
    "$ hdfs dfs -rm -r /request-code-counts\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Process Step (3.2)\n",
    "\n",
    "The request-code-reducer-count reducer file (see below) will give the text ouput of the counts. Use get_percentages_code() post processing to determine the percentage of each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informational responses (100–199)\t0.00%\n",
      "Successful responses (200–299)\t90.33%\n",
      "Redirection messages (300–399)\t3.74%\n",
      "Client error responses (400–499)\t5.93%\n",
      "Server error responses (500–599)\t0.00%\n"
     ]
    }
   ],
   "source": [
    "def get_percentages_code():\n",
    "    response_code_counts = {\n",
    "        \"Informational responses (100–199)\": 0,\n",
    "        \"Successful responses (200–299)\": 0, \n",
    "        \"Redirection messages (300–399)\": 0,\n",
    "        \"Client error responses (400–499)\": 0,\n",
    "        \"Server error responses (500–599)\": 0\n",
    "    }\n",
    "\n",
    "    with open('code_counts.txt', 'r') as c:\n",
    "        for line in c:\n",
    "            # print(line)\n",
    "            if line is not None or line != '\\n':\n",
    "                # varrying number of spaces so split on space and remove list elems that are empty/only spaces\n",
    "                line_list = line.strip().split(' ')\n",
    "                response_code, count = [item for item in line_list if item.strip() != \"\"]\n",
    "                try:\n",
    "                    response_code = int(response_code)\n",
    "                    count = int(count)\n",
    "                except ValueError:\n",
    "                    # count or response_code was not a number, so silently\n",
    "                    # ignore/discard this line\n",
    "                    continue\n",
    "                if 100 <= response_code < 200:\n",
    "                    response_code_counts[\"Informational responses (100–199)\"] += count\n",
    "                elif 200 <= response_code < 300:\n",
    "                    response_code_counts[\"Successful responses (200–299)\"] += count\n",
    "                elif 300 <= response_code < 400:\n",
    "                    response_code_counts[\"Redirection messages (300–399)\"] += count\n",
    "                elif 400 <= response_code < 500:\n",
    "                    response_code_counts[\"Client error responses (400–499)\"] += count\n",
    "                elif 500 <= response_code < 600:\n",
    "                    response_code_counts[\"Server error responses (500–599)\"] += count\n",
    "                else:\n",
    "                    print(f\"Unknown Code reached {response_code}\")\n",
    "\n",
    "                \n",
    "                total_reqs = sum(response_code_counts.values())\n",
    "\n",
    "    for response, count in response_code_counts.items():\n",
    "        percentage = (count / total_reqs) * 100\n",
    "        print(f\"{response}\\t{percentage:.2f}%\")\n",
    "\n",
    "get_percentages_code()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper and Reducer functions\n",
    "\n",
    "Here are the mapper and reducer functions used in map reduce processing. reducer function is pretty much the same except for changing variable names to account for request values vs return code values. mapper_code() used for debugging locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAPPER (3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "'''request-code-mapper.py'''\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    line = sys.stdin.readline()\n",
    "    pattern = re.compile(r'\\\" \\d{3}')\n",
    "    try:\n",
    "        while line:\n",
    "            match = pattern.findall(line)\n",
    "            if match:\n",
    "                method = match[0] # Extract the request method (e.g., GET, POST, HEAD)\n",
    "                print(f\"LongValueSum:{method}\\t1\")\n",
    "            line = sys.stdin.readline()\n",
    "                \n",
    "    except EOFError as error:\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)\n",
    "def mapper_code(debug=False):\n",
    "    # actually see what function returns\n",
    "    output_filename = 'mapper_log_code_output.txt'\n",
    "    with open(output_filename, 'w') as output_file:\n",
    "        with open('access.log.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                # line = sys.stdin.readline()\n",
    "                pattern = re.compile(r'\\ \\d{3}')\n",
    "                match = pattern.findall(line)\n",
    "                if match:\n",
    "                    response_code = match[0]  # Extract the request response_code (e.g., 200, 400, 401, 500)\n",
    "                    output_file.write(f\"{response_code}\\t1\\n\")\n",
    "                    # print(f\"{method}\\t1\")\n",
    "\n",
    "mapper_code(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REDUCER (3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "'''request-code-reducer-count.py'''\n",
    "import sys\n",
    "\n",
    "request_count = {}\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    code, count = line.strip().split('\\t', 1)\n",
    "    try:\n",
    "        count  = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if code not in request_count:\n",
    "        request_count[code] = 0\n",
    "\n",
    "    request_count[code] += count\n",
    "\n",
    "for code, count in request_count.items():\n",
    "    print(f\"{code}\\t{count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.3) What 5 IP addresses generate the most client errors\n",
    "\n",
    "For this question, I wrote my own mapper and reduce functions, `id-addr-mapper.py` and `id-addr-reducer.py`. The mapper will return each ip address that has a client error (400–499) along with a 1 for each row with that request type. This will be like: \\\n",
    "\n",
    "To use the log file, mapper, and reducer, I have to pull them into the repo that exists in the cluster.\n",
    "```console\n",
    "$ cd Quiz4\n",
    "$ git pull\n",
    "```\n",
    "\n",
    "Next, run the map reduce command\n",
    "\n",
    "```console\n",
    "$ mapred streaming -file ~/Quiz4/ip-addr-mapper.py ~/Quiz4/ip-addr-reducer.py \\\n",
    "-mapper ip-addr-mapper.py   \\\n",
    "-reducer ip-addr-reducer.py \\\n",
    "-input /user/cmeade/access.log.txt \\\n",
    "-output /ip_address_client_errors_counts\n",
    "```\n",
    "\n",
    "`/ip_address_client_errors_counts` will have the part output files\n",
    "```console\n",
    "$ hdfs dfs -ls /ip_address_client_errors_counts\n",
    "```\n",
    "Found 4 items \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop          0 2024-10-14 16:59 /ip_address_client_errors_counts/_SUCCESS \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop       7897 2024-10-14 16:59 /ip_address_client_errors_counts/part-00000 \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop       7887 2024-10-14 16:59 /ip_address_client_errors_counts/part-00001 \\\n",
    "-rw-r--r--   1 cmeade6479 hadoop       7643 2024-10-14 16:59 /ip_address_client_errors_counts/part-00002\n",
    "\n",
    "\n",
    "\n",
    "The counts are split acoss different parts (00000, 00001, and 00002). So, combine all the part output files and output that into a text file code_counts.\n",
    "```console\n",
    "$ hdfs dfs -text /ip_address_client_errors_counts/part* > ip_counts.txt\n",
    "```\n",
    "\n",
    "This ouputs a long file in comparison to the first two questions, the format will look like:\n",
    "\n",
    "```text\n",
    "<!-- ip_counts.txt -->\n",
    "LongValueSum:1.53.169.162       3\n",
    "LongValueSum:101.100.129.49     1\n",
    "LongValueSum:101.108.192.95     1\n",
    "LongValueSum:101.51.59.107      1\n",
    "LongValueSum:102.105.72.99      1\n",
    "LongValueSum:102.156.146.164    1\n",
    "LongValueSum:102.159.154.249    1\n",
    "LongValueSum:102.42.167.175     1\n",
    "LongValueSum:103.105.28.177     2\n",
    "LongValueSum:103.105.35.113     4\n",
    "LongValueSum:103.129.32.65      17\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "Clean up cluster space by deleting the directory since the txt file has been generated. Can also delete access log to clear space.\n",
    "```console\n",
    "$ hdfs dfs -rm -r /ip_address_client_errors_counts\n",
    "$ hdfs dfs -rm -r  /user/cmeade/access.log.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Process Step  (3.3)\n",
    "\n",
    "The ip-addr-reducer reducer file will give the text ouput of the counts. Use top_five_ips_client_errors() post processing to sort these counts and return the top 5. After post processing, it can be determined that the top 5 IPs with the most client errors (in descending order) are 173.255.176.5, 212.9.160.24, 13.77.204.88, 51.210.243.185, and 193.106.30.100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LongValueSum:173.255.176.5', 2059),\n",
       " ('LongValueSum:212.9.160.24', 126),\n",
       " ('LongValueSum:13.77.204.88', 78),\n",
       " ('LongValueSum:51.210.243.185', 58),\n",
       " ('LongValueSum:193.106.30.100', 53)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_five_ips_client_errors():\n",
    "    ip_counts = {}\n",
    "    with open('ip_counts.txt', 'r') as c:\n",
    "        for line in c:\n",
    "            # print(line)\n",
    "            if line is not None or line != '\\n':\n",
    "                # varrying number of spaces so split on space and remove list elems that are empty/only spaces\n",
    "                line_list = line.strip().split(' ')\n",
    "                ip, count = [item for item in line_list if item.strip() != \"\"]\n",
    "                count = int(count)\n",
    "                ip_counts[ip] = count\n",
    "                # total_reqs += count\n",
    "\n",
    "    sorted_ip_counts = sorted(ip_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_ip_counts[0:5]\n",
    "\n",
    "top_five_ips_client_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper and Reducer functions\n",
    "\n",
    "Here are the mapper and reducer functions used in map reduce processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAPPER (3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "'''id-addr-mapper.py'''\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    line = sys.stdin.readline()\n",
    "    pattern = re.compile(r'(?P<ip>\\d{1,3}(?:\\.\\d{1,3}){3}) .*\" [4][0-9][0-9] ')\n",
    "    try:\n",
    "        while line:\n",
    "            match = pattern.findall(line)\n",
    "            if match:\n",
    "                ip = match[0] # Extract the request method (e.g., GET, POST, HEAD)\n",
    "                print(f\"LongValueSum:{ip}\\t1\")\n",
    "            line = sys.stdin.readline()\n",
    "                \n",
    "    except EOFError as error:\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REDUCER (3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "'''id-addr-reducer-count.py'''\n",
    "import sys\n",
    "\n",
    "ip_count = {}\n",
    "\n",
    "\n",
    "for line in sys.stdin:\n",
    "    ip, count = line.strip().split('\\t', 1)\n",
    "    try:\n",
    "        count  = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if ip not in ip_count:\n",
    "        ip_count[ip] = 0\n",
    "\n",
    "    ip_count[ip] += count\n",
    "\n",
    "for ip, count in ip_count.items():\n",
    "    print(f\"{ip}\\t{count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Presidential Speeches [15 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we fetch the prez_speeches.zip file within the cluster\n",
    "\n",
    "```console\n",
    "$ wget https://raw.githubusercontent.com/singhj/big-data-repo/main/datasets/prez_speeches.zip\n",
    "```\n",
    "\n",
    "Then we unzip the speeches and put all directoires with .txt files into the GitHub repo under ~/Quiz4/uncompressed_speeches\n",
    "\n",
    "We then put these files into hdfs\n",
    "\n",
    "```console\n",
    "$ hadoop fs -mkdir /user/cmeade/prez_speeches\n",
    "$ hadoop fs -put ~/Quiz4/uncompressed_speeches /user/singhj/prez_speeches -- put this in the wrong dir accidentally\n",
    "```\n",
    "\n",
    "Confirm the files got there\n",
    "```console\n",
    "$ hdfs dfs -ls /user/singhj/prez_speeches\n",
    "```\n",
    "\n",
    "Found 43 items\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:23 /user/singhj/prez_speeches/adams\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/arthur\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/bharrison\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/buchanan\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/bush\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/carter\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/cleveland\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:23 /user/singhj/prez_speeches/clinton\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/coolidge\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/eisenhower\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/fdroosevelt\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/fillmore\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:23 /user/singhj/prez_speeches/ford\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:23 /user/singhj/prez_speeches/garfield\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/grant\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/gwbush\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/harding\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/harrison\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:23 /user/singhj/prez_speeches/hayes\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/hoover\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/jackson\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:20 /user/singhj/prez_speeches/jefferson\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:23 /user/singhj/prez_speeches/johnson\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/jqadams\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/kennedy\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/lbjohnson\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/lincoln\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:23 /user/singhj/prez_speeches/madison\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/mckinley\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/monroe\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/nixon\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:20 /user/singhj/prez_speeches/obama\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/pierce\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/polk\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:23 /user/singhj/prez_speeches/reagan\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:23 /user/singhj/prez_speeches/roosevelt\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/taft\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/taylor\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:21 /user/singhj/prez_speeches/truman\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/tyler\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:20 /user/singhj/prez_speeches/vanburen\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/washington\n",
    "drwxr-xr-x   - cmeade6479 hadoop          0 2024-10-15 22:22 /user/singhj/prez_speeches/wilson\n",
    "\n",
    "\n",
    "With all the files loaded into hdfs, now the directory can be passed in as input to the MR command.\n",
    "\n",
    "```console\n",
    "$ mapred streaming -file ~/Quiz4/prez_speeches_mapper.py ~/Quiz4/prez_speeches_reducer.py \\\n",
    "-mapper prez_speeches_mapper.py   \\\n",
    "-reducer prez_speeches_reducer.py \\\n",
    "-input /user/singhj/prez_speeches/*/* \\\n",
    "-output /president_speech_valence/all_prez\n",
    "```\n",
    "\n",
    "\n",
    "If running on an individual president, the command would be:\n",
    "\n",
    "```console\n",
    "$ mapred streaming -file ~/Quiz4/prez_speeches_mapper.py ~/Quiz4/prez_speeches_reducer.py \\\n",
    "-mapper prez_speeches_mapper.py   \\\n",
    "-reducer prez_speeches_reducer.py \\\n",
    "-input /user/singhj/prez_speeches/<president last name>/<president last name> \\\n",
    "-output /president_speech_valence/<president last name>\n",
    "```\n",
    "\n",
    "`/president_speech_valence/all_prez` will have the part output files\n",
    "```console\n",
    "$ hdfs dfs -ls /president_speech_valence/all_prez\n",
    "```\n",
    "\n",
    "\n",
    "The mapper outputs split acoss different parts. So, combine all the part output files and output that into a result txt file.\n",
    "```console\n",
    "$ hdfs dfs -text /president_speech_valence/<president last name>/part* > <president name>_results.txt\n",
    "OR (to get all presidents)\n",
    "$ hdfs dfs -text /president_speech_valence/all_prez > president_results.txt\n",
    "\n",
    "```\n",
    "hdfs dfs -text /president_speech_valence/harrison_2/part* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Process Step (4)\n",
    "The mapper will print output of the form\n",
    "(president name, valence score) \n",
    "adams\t-2\n",
    "\n",
    "The reducer will give output parts that when combined together will look like: \\\n",
    "\"{predident_name}\\ttotal\\t{total_valence_words}\"\n",
    "adams   total   93\n",
    "\n",
    "\"{name}\\t{net_v_score}\"\n",
    "adams   53\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper and Reducer functions (4)\n",
    "\n",
    "Here are the mapper and reducer functions used in map reduce processing. Maintain a dict with valence words and values. Clean text and remove stop words. Ouput president name and corresponding valence score for each valence word used. Reducer will take in valence output and get the net valence score and the total number of valence words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAPPER (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "'''prez_speeches_mapper.py'''\n",
    "import sys\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import os\n",
    "\n",
    "stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "stopwords = list(set(stopwords_list.decode().splitlines()))\n",
    "valence_words_data = requests.get(\"https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-en-165.txt\").content\n",
    "valencewords = list(set(valence_words_data.decode().splitlines()))\n",
    "valence_dict = {}\n",
    "\n",
    "for v in valencewords:\n",
    "    word, score = v.split('\\t')\n",
    "    valence_dict[word] = score\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    list_ = re.sub(r\"[^a-zA-Z0-9]\", \" \", words.lower()).split()\n",
    "    return [itm for itm in list_ if itm not in stopwords]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('[\\d\\n]', ' ', text)\n",
    "    return ' '.join(remove_stopwords(text))\n",
    "\n",
    "def valence(text):\n",
    "    return calc_valence(text)\n",
    "\n",
    "def calc_valence(text):\n",
    "    v = []\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode('utf-8')\n",
    "    text_list = text.split(' ')\n",
    "    for w in text_list:\n",
    "        if w in valence_dict:\n",
    "            v.append(valence_dict[w])\n",
    "\n",
    "    return v\n",
    "\n",
    "# def main(argv):\n",
    "def main(argv):\n",
    "    president_name = 'missing prez name'\n",
    "    line = sys.stdin.readline()\n",
    "    try:\n",
    "        while line:\n",
    "            clean_line = clean_text(line) # returns a line as a space-seperated line, or a sentence if you will\n",
    "            # fetch president name\n",
    "            if \"mapreduce_map_input_file\" in os.environ:\n",
    "                president_file_name = os.environ['mapreduce_map_input_file']\n",
    "                president_name = re.sub(r'.*/|_speeches_\\d+\\.txt$', '', president_file_name)\n",
    "            valence_vals = valence(clean_line)\n",
    "            for v in valence_vals:\n",
    "                print(f\"{president_name}\\t{v}\")\n",
    "            line = sys.stdin.readline()\n",
    "    except EOFError as error:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REDUCER (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7020/3501790968.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvalence_words_ct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# with open('adams.txt', 'r') as c:  # debug local file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpresident_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalence_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "'''prez_speeches_reducer.py'''\n",
    "import sys\n",
    "\n",
    "valence_aggregate = {}\n",
    "valence_words_ct = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "# with open('adams.txt', 'r') as c:  # debug local file\n",
    "    president_name, valence_score = line.strip().split('\\t', 1)\n",
    "    try:\n",
    "        valence_score  = int(valence_score)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    if president_name not in valence_aggregate:\n",
    "        valence_aggregate[president_name] = 0\n",
    "\n",
    "    valence_aggregate[president_name] += valence_score\n",
    "    \n",
    "    if president_name not in valence_words_ct:\n",
    "        valence_words_ct[president_name] = 0\n",
    "\n",
    "    valence_words_ct[president_name] += 1\n",
    "\n",
    "for name, total_valence_words in valence_words_ct.items():\n",
    "    print(f\"{name}\\t{total_valence_words}\")\n",
    "\n",
    "for name, net_v_score in valence_aggregate.items():\n",
    "    print(f\"{name}\\ttotal\\t{net_v_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) calc_valence(text) is a function that you write. Be sure to test this function under any imaginable conditions, for example:\n",
    "- When text is empty,\n",
    "- When text is a string of non-printable characters,\n",
    "- When text is a bytecode string,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valence(text):\n",
    "    return calc_valence(text)\n",
    "\n",
    "def calc_valence(text):\n",
    "    v = []\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode('utf-8')\n",
    "    text_list = text.split(' ')\n",
    "    for w in text_list:\n",
    "        if w in valence_dict:\n",
    "            v.append(valence_dict[w])\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\u0001\u0002\u0003\u0004\u0005\u0006\t\n",
      "\u000e\u000f\u0010\u0011\u0012\u0013\u0014\u0015\u0016\u0017\u0018\u0019\u001a\u001b\u001c\u001d\u001e\u001f\n",
      "b'foobar'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_valence('''<title=\"Address to Congress on Yalta\">\n",
    "<date=\"March 1, 1945\">\n",
    "I hope that you will pardon me for this unusual posture of sitting down during the presentation of what I want to say, but I know that you will realize that it makes it a lot easier for me not to have to carry about ten pounds of steel around on the bottom of my legs; and also because of the fact that I have just completed a fourteen-thousand-mile trip.\n",
    "First of all, I want to say, it is good to be home.\n",
    "It has been a long journey. I hope you will also agree that it has been, so far, a fruitful one.''')\n",
    "# calc_valence on empty string\n",
    "calc_valence('') # works, returns nothing\n",
    "# calc_valence on string of non printable chars\n",
    "non_printable = ''.join(chr(i) for i in range(32))\n",
    "print(non_printable)\n",
    "calc_valence(non_printable) # works, returns nothing\n",
    "# calc_valence on a bytecode string\n",
    "byte_str = b'foobar'\n",
    "print(byte_str)\n",
    "calc_valence(byte_str) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2 points] How much data, in bytes, was emitted by the mappers?\n",
    "\n",
    "When running on Taft's speeches, 252743 bytes were outputted by the mappers\n",
    "\n",
    "When running on all speeches,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hadoop Errors [15 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Where (what server & location) did the divide-by-zero error messages show up and how many did you find?\n",
    "\n",
    "MR command:\n",
    "\n",
    "```console\n",
    "mapred streaming -files ~/Quiz4/mapper_noll_part5.py ~/big-data-repo/hadoop/reducer_noll.py \\\n",
    "-mapper mapper_noll_part5.py   \\\n",
    "-reducer reducer_noll.py \\\n",
    "-input /user/singhj/five-books \\\n",
    "-output /books-counts-part-5\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAPPER (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "'''mapper_noll_part5.py'''\n",
    "import sys, re\n",
    "import random\n",
    "\n",
    "def main(argv):\n",
    "    line = sys.stdin.readline()\n",
    "    pattern = re.compile(\"[a-zA-Z][a-zA-Z0-9]*\")\n",
    "    try:\n",
    "        while line:\n",
    "            x = 1 / random.randint(0,99)\n",
    "            for word in pattern.findall(line):\n",
    "                print (\"LongValueSum:\" + word.lower() + \"\\t\" + \"1\")\n",
    "                # x = 1 / random.randint(0,99)\n",
    "            line = sys.stdin.readline()\n",
    "    except EOFError as error:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 How many such messages did you find? Is the count you found consistent with what you might expect from random.randint(0,99)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
